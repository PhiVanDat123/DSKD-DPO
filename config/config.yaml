# random seed for batch sampling
seed: 0

#mode
policy_mode: student
reference_mode: teacher

# number of iterators in training
train_iterator: 100

# the batch size for training; for FSDP, the batch size per GPU is batch_size / (grad_accumulation_steps * num_gpus)
batch_size: 2

# the batch size during evaluation and sampling, if enabled
eval_batch_size: 16

# debug mode (disables wandb, model checkpointing, etc.)
debug: false

# the port to use for FSDP
fsdp_port: null

# which dataset(s) to train on; can pass a list like datasets=[hh,shp]
datasets: "pvdhihihi/tis-dpo-5k"

base_data_dir: null

reverse_dataset: false

# wandb configuration
wandb:
  enabled: true
  entity: null
  project: "tis-dpo"

# to create the local run directory and cache models/datasets,
#   we will try each of these directories in order; if none exist,
#   we will create the last one and use it

#output_dir: "output"

# whether or not to generate samples during evaluation; disable for FSDP/TensorParallel
#   is recommended, because they are slow
sample_during_eval: false

# how many model samples to generate during evaluation
n_eval_model_samples: 16

# whether to eval at the very beginning of training
do_first_eval: true

# an OmegaConf resolver that returns the local run directory, calling a function in utils.py
local_run_dir: ${get_local_run_dir:${exp_name},${output_dir}}

# name for this experiment in the local run directory and on wandb
exp_name: ${build_exp_name:${loss.name},${model.policy_name_or_path},${datasets},${reverse_dataset},${model.reference_name_or_path}}

# the learning rate
lr: 5e-7

# number of steps to accumulate over for each batch
#   (e.g. if batch_size=4 and gradient_accumulation_steps=2, then we will
#   accumulate gradients over 2 microbatches of size 2)
gradient_accumulation_steps: 1

# the maximum gradient norm to clip to
max_grad_norm: 10.0

# the maximum allowed length for an input (prompt + response)
max_length: 512

# the maximum allowed length for a prompt
max_prompt_length: 256

# the number of epochs to train for; if null, must specify n_examples
n_epochs: 1

# the number of examples to train for; if null, must specify n_epochs
n_examples: null

# the number of examples to evaluate on (and sample from, if sample_during_eval is true)
n_eval_examples: 256

# the trainer class to use (e.g. BasicTrainer, FSDPTrainer, TensorParallelTrainer)
trainer: BasicTrainer

# The optimizer to use; we use RMSprop because it works about as well as Adam and is more memory-efficient
optimizer: RMSprop

# number of linear warmup steps for the learning rate
warmup_steps: 150

# whether or not to use activation/gradient checkpointing
activation_checkpointing: false

# evaluate and save model every eval_every steps
eval_every: 100_000

# prevent wandb from logging more than once per minimum_log_interval_secs
minimum_log_interval_secs: 1.0

# use label smoothing for the loss function
label_smoothing: 0.1

# args for various_divergence
kd_rate: 0.5
kd_temperature: 1.0
teacher_temperature: 1.0
kd_objective: "forward_kl"
adaptive_kl_alpha: 0.9
skew_lambda: 0.7
report_logits: false

# args for distiller
#teacher_model_path: "Qwen/Qwen2.5-14B"
#teacher_model_type: "qwen"
#model_path: "meta-llama/Llama-3.1-8B"
#model_type: "llama"
reference_name_or_path: "Qwen/Qwen2.5-14B"
teacher_model_type: "qwen"
policy_name_or_path: "meta-llama/Llama-3.1-8B"
model_type: "llama"
model_dtype: "fp32" 
projector_config_path: '/home/hungpv/projects/DSKD-DPO/config/projector/projector_config.json'
teacher_to_student_token_mapping: None
teacher_to_student_id_mapping: None
peft: None
peft_path: None
do_train: true
peft_lora_r: 8
peft_lora_alpha: 16
peft_lora_dropout: 0.1
gradient_checkpointing: false
projector_lr: 1e-4
pretrained_projector: None
pretrained_projector_lr: 1e-4
is_model_parallel: false
n_embed: 4096
hidden_size: 8192`
teacher_peft_path: None
projector_path: None

projector_warmup_steps: 1000

#weights
split: "train"
num_gpus: 2
force_sequential: false
positive_model_name: "tonyshelby/Qwen2.5_0.5B_SFT_sample"
negative_model_name: "openai-community/gpt2"
output_dir: "generated-data/ultra-feedback-tisdpo"
data_path: "tonyshelby/ultra-feedback_checking"

defaults:
- _self_
- model: sft # basic model configuration
- loss: sft # which loss function, either sft or dpo (specify loss.beta if using dpo)
#- transform_config: default # which transform configuration to use
#- transform@transform: default 